# **Big Data Praxisleistung: Use Spotify Audio Features To Categorize Music**

**Autor:** Vanessa Wettori  
**Institution:** DHBW Stuttgart

Der ETL-Workflow nutzt Apache Airflow, um Daten aus der Spotify-API zu extrahieren, diese zu verarbeiten und anschlieÃŸend in einer MongoDB-Datenbank zu speichern. Der Nutzer gibt eine Spotify-Playlist-ID ein, anhand der die enthaltenen Tracks geladen werden. FÃ¼r jeden Track werden zusÃ¤tzlich die zugehÃ¶rigen Audio-Features abgerufen. Auf Basis dieser Audio-Features werden die Songs automatisch einer Kategorie wie beispielsweise HipHop zugeordnet. Die Ergebnisse lassen sich anschlieÃŸend Ã¼ber eine WeboberflÃ¤che Ã¼bersichtlich anzeigen.

## ğŸ—‚ **Inhaltsverzeichnis**


- [Projektstruktur](#Projektstruktur)
- [Airflow](#Airflow)
- [Weitere verwendete Skripte](#Weitere verwendete Skripte)
- [AusfÃ¼hrung](#AusfÃ¼hrung)

## Projektstruktur

Von nicht relevanten Ordnern wurde darauf verzichtet, die Struktur und Inhalte anzugeben.
Die Projektstrutur sieht wie folgt aus:

```bash
â”œâ”€Â Dockerfile.airflow
â”œâ”€Â Dockerfile.flask
â”œâ”€Â README.md
â”œâ”€Â app.py
â”œâ”€Â dags
â”‚Â Â â”œâ”€Â __pycache__
â”‚Â Â â””â”€Â track_categorization.py
â”œâ”€Â data
â”‚Â Â â””â”€Â mongodb/
â”œâ”€Â docker-compose.yml
â”œâ”€Â playlist
â”‚Â Â â””â”€Â playlist_id.txt
â”œâ”€Â plugins/
â”œâ”€Â requirements.tx
â”œâ”€Â set_playlist_id.py
â”œâ”€Â startup.sh
â””â”€Â templates
Â Â Â â””â”€Â index.html
```

### ErklÃ¤rung der Struktur

- **`Dockerfile.airflow`**: Die Konfiguration fÃ¼r das Airflow-Docker-Image
- **`Dockerfile.flask`**: Die Konfiguration fÃ¼r das Flask-Docker-Image
- **`README.md`**: Die Dokumentation des Projekts.
- **`app.py`**: - Die Datei erstellt eine Flask-Webanwendung, die eine Verbindung zu einer MongoDB-Datenbank herstellt und eine HTML-Seite rendert, die alle Tracks aus der Datenbank anzeigt.
- **`dags/`**: EnthÃ¤lt die DAG die fÃ¼r den Workflow zustÃ¤ndig ist.
  - **`track_categorization`**: EnthÃ¤lt den Airflow-DAG um die Daten von Spotify zu ziehen und verarbeitet diese weiter.
- **`data/`**: Verzeichnis fÃ¼r die Daten:
  - **`mongodb/`**: JSON mit Key fÃ¼r die Kaqqle API.
- **`docker-compose.yml`**: Verwaltet die verschiedenen Container.
- **`playlist`**: Ordner, welches eine Textdatei mit der Playlist-Id enthÃ¤lt, die vom User eingegeben wurde.
- **`plugins/`**: Eine Erweiterungen fÃ¼r Airflow
- **`requirements.txt`**: Bibliotheken die fÃ¼r das Projekt benÃ¶tigt werden.
- **`set_playlist_id.py`**: Skript, mit dem der User die Playlist-Id setzen kann.
- **`startup.sh`**: Startet den Airflow-Scheduler und den Webserver.
-**`templates`**: EnthÃ¤lt das Frontend, um die Ergebnisse aus MongoDB visuell darzustellen.
  -**`index.html`**: Frontend fÃ¼r die Ergenisee

## **Airflow**

Die Airflow-DAG besteht aus 6 Tasks, die im Folgenden weiter erklÃ¤rt werden. Die Reihenfolge entspricht der hier aufgefÃ¼hrten Nummerierung.

### **Ablauf des Workflows**

1. **Erstellen und leeren der Verzeichnisse: clear_directorys**

   - Die Task `clear_directorys` ist ein BashOperator, der vorhandene Verzeichnisse lÃ¶scht undneue Verzeichnisse auf dem HDFS (Hadoop Distributed File System)  erstellt, um sicherzustellen, dass alte Daten den Workflow nicht beeinflussen.

2. **Leeren der Datenbank: clear_database**

   - Die Task `clear_database` ist ein PythonOperator, der in der Airflow-DAG definiert ist. Er ruft die Funktion `clear_mongo_data` auf, um die tracks-Kollektion in der MongoDB-Datenbank zu lÃ¶schen. Dies stellt sicher, dass alte Daten entfernt werden und die Datenbank fÃ¼r neue Daten vorbereitet ist.

3. **Herunterladen der Track-IDs: get_playlist_tracks**

   - Die Task `get_playlist_tracks` ist ein BashOperator, der die Spotify-API aufruft, um die Tracks einer Playlist abzurufen. Die Ergenisse werden als JSON-Datei im HDFS (Hadoop Distributed File System) gespeichert. Dabei wird zunÃ¤chst ein Zugriffstoken von der Spotify-API angefordert und anschlieÃŸend die Tracks der Playlist abgerufen. Die Playlist-ID wurde vor Containerstart von dem User eingegeben und wird zum Start der ETL aus dem Textdatei in eine Variable eingelesen, die Task verwendet.

4. **Herunterladen der Audiofeatures fÃ¼r die zuor gefundenen Tracks: get_audio_features**

   - Die Task `get_audio_features` ist ein PythonOperator, der die Funktion `get_for_each_track_audio_features` aufruft. Diese Funktion dient dazu, die Audio-Features fÃ¼r jeden Track einer Playlist abzurufen und als JSON-Dateien im HDFS zu speichern. Dabei liest `get_for_each_track_audio_features` die Track-IDs aus einer JSON-Datei, die die Playlist-Daten enthÃ¤lt, und ruft die entsprechenden Audio-Features Ã¼ber die Spotify-API ab.

5. **Speichern im finalen Verzeichnis: save_to_final_path**

   - Die Task `save_to_final_path` ist ein PythonOperator, der die Funktion `save_tracks_to_final_path` aufruft, um die abgerufenen Track- und Audio-Feature-Daten zu verarbeiten und die bereinigten Daten als Parquet-Dateien im HDFS zu speichern.

6. **Kategorien berechnen und in Datenbank speichern: calculate_category_and_save_to_db**

   - Die Task `calculate_category_and_save_to_db` ist ein PythonOperator, der die Funktion calculate_category aufruft, um die Audio-Features der Tracks zu analysieren. Die Tracks werden mithilfe von Apache Spark, in verschiedene Kategorien eingeordnet und die Ergebnisse in einer MongoDB-Datenbank zu gespeichert. Diese ist die endgÃ¼ltige Datenbank.

## **Weitere verwendete Skripte**

Wie schon in dem Kaptiel Ordnerstruktur erwÃ¤hnt, gibt es noch zwei weitere Python-Skripte, die die Funktionweise von der ETL sicherstellen. Sie sind fÃ¼r die AusfÃ¼hrung und Darstellung der Ergebnisse wichtig.

1. **`set_playlist_id.py`**:
   - Das Skript ermÃ¶glicht es dem Benutzer, eine Playlist-ID einzugeben und diese in einer Datei zu speichern. Diese Playlist-ID wird spÃ¤ter von der Airflow-DAG verwendet, um die Tracks der angegebenen Playlist von der Spotify-API abzurufen. Dazu erstellt das Skript eine Text-Datei, die die Playlist-ID enthÃ¤lt. Diese wird in dem Verzeichnis `Playlist` gespeichert, worauf die DAG zugriff hat und dieses auslesen kann.
  
2. **`app.py`**:
   -Das Skript erstellt eine Flask-Webanwendung, die eine Verbindung zu einer MongoDB-Datenbank herstellt und eine HTML-Seite rendert, die alle Tracks aus der Datenbank anzeigt. Somit wird dadurch die WeboberflÃ¤che bereitgestellt und sichergestellt, dass eine Verbindung zu der Datenbank besteht.

## **AusfÃ¼hrung**

1. **Voraussetzungen:**

   - Docker und Docker-Compose installiert

2. **Schritte zur AusfÃ¼hrung:**

```bash
git clone https://github.com/Vanilla-1311/BigDataSpotify.git
cd BigData
```

Nach diesen Befehlen muss zunÃ¤chst `set_playlist_id.py` ausgefÃ¼hrt werden. Dies geschieht mithilfe diesen Befehls:

```bash
python set_playlist_id.py
```

Hier wird man aufgefordert, die Playlist-ID einzugeben. Danach kann der Conatiner gestartet werden.

```bash
docker-compose up
```

### 3. Hadoop Cluster starten

```bash
docker exec -it hadoop bash
sudo su hadoop
start-all.sh
```

### 4. Airflow-API Ã¶ffnen

Auf `localhost:8080` kann die Airflow-Api jetzt geÃ¶ffnet werden, um die DAG zu starten.

### 5. Flask-Frontend Ã¶ffnen

Im Browser unter `localhost:5000` kann nach der Vollendigung der Aufgabe, die Ergebnisse betrachtet werden. 